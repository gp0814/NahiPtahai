{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423d5fa-3d02-49a6-abe8-5ec546d84551",
   "metadata": {},
   "outputs": [],
   "source": [
    "##program1\n",
    "#pip install gensim\n",
    "import gensim.downloader as api\n",
    "# Load pre-trained model\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# Example words\n",
    "word1 = \"king\"\n",
    "word2 = \"man\"\n",
    "word3 = \"woman\"\n",
    "# Performing vector arithmetic\n",
    "result_vector = model[word1] - model[word2] + model[word3]\n",
    "predicted_word = model.most_similar([result_vector], topn=2)\n",
    "print(f\"Result of '{word1} - {word2} + {word3}' is: {predicted_word[1][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57348af0-0490-4bd9-bfcb-466ee90bf04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program2\n",
    "#conda install -c conda-forge matplotlib\n",
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "# Load pre-trained Word2Vec model\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# Select 10 words from a specific domain (e.g., technology)\n",
    "words = [\"computer\", \"internet\", \"software\", \"hardware\", \"disk\", \"robot\", \"data\",\n",
    "\"network\", \"cloud\", \"algorithm\",\"india\",\"england\",\"pakistan\"]\n",
    "# Get word vectors and convert to a 2D NumPy array\n",
    "word_vectors = np.array([model[word] for word in words])\n",
    "# Reduce dimensions using PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_vectors = pca.fit_transform(word_vectors)\n",
    "# Plot PCA visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, word in enumerate(words):\n",
    "    plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1])\n",
    "    plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]))\n",
    "plt.title(\"PCA Visualization of Word Embeddings (Technology Domain)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.show()\n",
    "input_word = \"computer\" # You can change this to any word in your list\n",
    "similar_words = model.most_similar(input_word, topn=5)\n",
    "print(f\"Words similar to '{input_word}':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec995f4-b3ee-4fd6-a46c-bbe633bf6365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program 3\n",
    "#pip install gensim nltk\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Sample domain-specific dataset (Medical domain)\n",
    "corpus = [\"A patient with diabetes requires regular insulin injections.\",\n",
    "\"Medical professionals recommend exercise for heart health.\",\n",
    "\"Doctors use MRI scans to diagnose brain disorders.\",\n",
    "\"Antibiotics help fight bacterial infections but not viral infections.\",\n",
    "\"The surgeon performed a complex cardiac surgery successfully.\",\n",
    "\"Doctors and nurses work together to treat patients.\",\n",
    "\"A doctor specializes in diagnosing and treating diseases.\"\n",
    "]\n",
    "# Tokenize sentences\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "# Train Word2Vec model (Using Skip-gram for better results)\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=3, min_count=1,workers=4, sg=1)\n",
    "# Save the model\n",
    "model.save(\"medical_word2vec.model\")\n",
    "# Test trained model - Find similar words\n",
    "similar_words = model.wv.most_similar(\"doctor\", topn=5)\n",
    "# Display results\n",
    "print(\"Top 5 words similar to 'doctor':\")\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87320db4-1ed1-4166-ab23-d5fb7be12cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program4\n",
    "#pip install gensim openai nltk\n",
    "from transformers import pipeline\n",
    "import gensim.downloader as api\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "word = \"technology\"\n",
    "similar_words = glove_model.most_similar(word, topn=5)\n",
    "print(f\"Similar words to '{word}': {similar_words}\")\n",
    "# Load a text generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "# Function to generate text\n",
    "def generate_response(prompt, max_length=100):\n",
    "    response = generator(prompt, max_length=max_length, num_return_sequences=1)\n",
    "    return response[0]['generated_text']\n",
    "# Original prompt\n",
    "original_prompt = \"Explain the impact of technology on society.\"\n",
    "original_response = generate_response(original_prompt)\n",
    "# Enriched prompt\n",
    "enriched_prompt = \"Explain the impact of technology, innovation, science, engineering, and digital advancements on society.\"\n",
    "enriched_response = generate_response(enriched_prompt)\n",
    "# Print responses\n",
    "print(\"Original Prompt Response:\")\n",
    "print(original_response)\n",
    "print(\"\\nEnriched Prompt Response:\")\n",
    "print(enriched_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00ef7b-7bd0-4827-8910-2aabf6992adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program 5\n",
    "#pip install gensim nltk\n",
    "import gensim.downloader as api\n",
    "# Load GloVe embeddings directly\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# Function to construct a short paragraph\n",
    "def construct_paragraph(seed_word, similar_words):\n",
    "# Create a simple template-based paragraph\n",
    "    paragraph = (\n",
    "    f\"In the spirit of {seed_word}, one might embark on an unforgettable {similar_words[0][0]}\"f\"to distant lands. Every {similar_words[1][0]} brings new challenges and opportunitiesfor{similar_words[2][0]}. \"\n",
    "    f\"Through perseverance and courage, the {similar_words[3][0]} becomes a tale of triumph,muchlike an {similar_words[4][0]}.\"\n",
    "    )\n",
    "    return paragraph\n",
    "# Generate a paragraph for \"adventure\"\n",
    "seed_word = \"adventure\"\n",
    "similar_words = model.most_similar(seed_word, topn=5)\n",
    "# Construct a paragraph\n",
    "paragraph = construct_paragraph(seed_word, similar_words)\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0a300-ab4c-40f0-b165-96ea74d74872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program 6\n",
    "#pip install transformers torch\n",
    "from transformers import pipeline\n",
    "# Load the sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "# Example sentences\n",
    "sentences = [\n",
    "\"I love this product! It works perfectly.\",\n",
    "\"This is the worst experience I've ever had.\",\n",
    "\"The weather is nice today.\",\n",
    "\"I feel so frustrated with this service.\"\n",
    "]\n",
    "# Analyze sentiment for each sentence\n",
    "results = sentiment_pipeline(sentences)\n",
    "# Print the results\n",
    "for sentence, result in zip(sentences, results):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Sentiment: {result['label']}, Confidence:{result['score']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d006bb3-9614-4007-8878-c358a9e6659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program 7\n",
    "# pip install transformers torch\n",
    "from transformers import pipeline\n",
    "# Load the T5 summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
    "# Example passage\n",
    "passage = \"Machine learning is a subset of artificial intelligence that focuses on training algorithmsto make predictions. It is widely used in industries like healthcare, finance, and retail.\"\n",
    "# Generate the summary\n",
    "summary = summarizer(passage, max_length=30, min_length=10, do_sample=False)\n",
    "# Print the summarized text\n",
    "print(\"Summary:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b658e6c3-e642-4a8d-aefb-f30cff4c3e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program 8\n",
    "#!pip install langchain langchain-community cohere google-auth google-auth-oauthlib google-auth-httplib2googleapiclient\n",
    "import os\n",
    "from cohere import Client\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Step 1: Set Cohere API Key\n",
    "os.environ[\"COHERE_API_KEY\"] = \"opKHtxN5LG2MLT9RGj2vanYfhTq4Y7TCS2qGkjPn\"\n",
    "co = Client(os.getenv(\"COHERE_API_KEY\"))\n",
    "# Step 2: Load Text Document (Option 1: Local File)\n",
    "with open(\"Narendra modi.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text_document = file.read()\n",
    "# Step 3: Create a Prompt Template\n",
    "template = \"\"\"\n",
    "You are an expert summarizer. Summarize the following text in a concise manner:\n",
    "Text: {text}\n",
    "Summary:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"text\"], template=template)\n",
    "formatted_prompt = prompt_template.format(text=text_document)\n",
    "# Step 4: Send Prompt to Cohere API\n",
    "response = co.generate(\n",
    "model=\"command\",\n",
    "prompt=formatted_prompt,\n",
    "max_tokens=50\n",
    ")\n",
    "# Step 5: Display Output\n",
    "print(\"Summary:\")\n",
    "print(response.generations[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa47ba-f1e6-40be-a602-197e1faa745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program 9\n",
    "#!pip install langchain pydantic wikipedia\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import wikipedia\n",
    "# Step 1: Define the Pydantic Schema for the Output\n",
    "class InstitutionDetails(BaseModel):\n",
    "    name: str = Field(description=\"Name of the institution\")\n",
    "    founder: Optional[str] = Field(description=\"Founder of the institution\")\n",
    "    founding_year: Optional[int] = Field(description=\"Year the institution wasfounded\")\n",
    "    branches: Optional[int] = Field(description=\"Number of branches of theinstitution\")\n",
    "    employees: Optional[int] = Field(description=\"Number of employees in theinstitution\")\n",
    "    summary: Optional[str] = Field(description=\"Summary of the institution\")\n",
    "# Step 2: Fetch Institution Details from Wikipedia\n",
    "def fetch_institution_details(institution_name: str) -> InstitutionDetails:\n",
    "    try:\n",
    "        # Fetch the Wikipedia page\n",
    "        page = wikipedia.page(institution_name)\n",
    "        summary = wikipedia.summary(institution_name, sentences=3)\n",
    "        # Extract details (this is a simple example; you may need to refine this)\n",
    "        details = {\n",
    "        \"name\": institution_name,\n",
    "        \"founder\": None, # You can use NLP or regex to extract this from the page content\n",
    "        \"founding_year\": None, # Extract from the page content\n",
    "        \"branches\": None, # Extract from the page content\n",
    "        \"employees\": None, # Extract from the page content\n",
    "        \"summary\": summary,\n",
    "        }\n",
    "    # Parse the details into the Pydantic schema\n",
    "        return InstitutionDetails(**details)\n",
    "    except Exception as e:\n",
    "        return InstitutionDetails(name=institution_name, founder=None,founding_year=None, branches=None, employees=None,summary=\"Error parsingdetails.\")\n",
    "# Step 3: Invoke the Chain and Fetch Results\n",
    "if __name__ == \"__main__\":\n",
    "    institution_name = input(\"Enter the institution name: \")\n",
    "    details = fetch_institution_details(institution_name)\n",
    "    print(details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d7f7f-59b0-4955-92c4-7cf50e97e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Program 10\n",
    "#ollama pull deepseek-r1:1.5b\n",
    "#pip install pdfplumber\n",
    "#pip install ollama\n",
    "import pdfplumber\n",
    "import ollama\n",
    "\n",
    "# Load and extract text from PDF\n",
    "with pdfplumber.open(\"Documents/indian-penal-code-ncib.pdf\") as pdf:\n",
    "    full_text = \"\\n\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "\n",
    "# Chat loop\n",
    "print(\"IPC Chatbot Ready. Ask anything about the Indian Penal Code.\")\n",
    "while True:\n",
    "    query = input(\"\\nYou: \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    prompt = f\"\"\"You are a helpful assistant.\n",
    "{full_text[:6000]}  # limit text to avoid going over context length\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = ollama.chat(model=\"deepseek-r1:1.5b\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    print(f\"Chatbot: {response['message']['content']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
